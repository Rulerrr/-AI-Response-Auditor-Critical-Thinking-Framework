# AI Response Auditor: Critical Thinking Framework

Developed by **Ruslan Krucheniuk** | M√°laga, Spain

## üéØ Why This Exists

Large Language Models (LLMs) are optimised for "helpfulness" and "fluency," which often leads to a phenomenon known as "polite hallucination." AI tends to avoid conflict, seeks middle ground where none exists, and mirrors user biases. 

This framework is a pragmatic tool designed to audit AI outputs using classical logic, system paradoxes, and cognitive science. It helps you see through the "confident eloquence" of the machine.

---

## ‚ö†Ô∏è Common AI Logical Fallacies

When auditing an AI response, look for these common "glitches":

* **The False Golden Mean:** The AI attempts to find a compromise between a factual truth and a popular myth just to remain "neutral".
* **The Slippery Slope:** Generating catastrophic predictions without a clear causal chain.
* **Ad Hominem Deflection:** The AI questions the user's intent or ethics to avoid answering a complex technical or logical challenge.
* **Appeal to Consensus:** Justifying a conclusion because it's "widely accepted," ignoring fringe but scientifically valid data.

---

## üõ† The Audit Toolbox (Quick Check)

| Detected Pattern | Counter-Logic (Paradox) | The Auditor's Question |
| :--- | :--- | :--- |
| Focus on local efficiency | **Braess Paradox** | Will adding this resource actually degrade the entire system's performance? |
| Obsession with metrics | **Goodhart's Law** | If we follow this metric, how will agents "game" the system to look good? |
| Claims efficiency = less waste | **Jevons Paradox** | Will increased efficiency lead to higher total consumption in the long run? |
| Over-reliance on automation | **Automation Paradox** | How will the system survive when the automation fails and human skills have degraded? |
| Forced compromise | **False Middle Ground** | Is this a factual conclusion or just a diplomatic "safe" response? |

---

## üîç Verification Protocol

To properly audit an AI, follow these three steps:

1. **The Socratic Drill:** Ask "Why?" three times to expose if the logic is deep or just a surface-level word association.
2. **Boundary Testing (Popper's Principle):** Explicitly ask the AI to find a fatal flaw in its own previous response.
3. **Bias Extraction:** Ask the AI to rewrite the answer from the perspective of a radical pragmatist or a cynical expert to see how the content shifts.

---

## üìú License & Attribution

This project is licensed under the **MIT License**. 

**Author:** Ruslan Krucheniuk
**Location:** M√°laga, Spain
**Year:** 2023-2026

*Note: This framework is built for those who seek the truth, not just a convenient argument.*
